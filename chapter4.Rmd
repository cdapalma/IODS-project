# Chapter 4 - Assignment 4


Analysis Exercise

I've loaded the data from the MASS package. In the exercise it didn't mention the data name, so I assumed we will be using the Boston dataset from MASS, as we did in the Exercise 4.

I have explored the structure and dimension of the data.
I can observe that this dataset has 506 observations and 14 variables.
I used the corrplot to visualize the correlation between variables of the Boston dataset.
The 14 variables are described as followed:

* crim: per capita crime rate by town.
* zn: proportion of residential land zoned for lots over 25,000 sq.ft.
* indus: proportion of non-retail business acres per town.
* chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
* nox: nitrogen oxides concentration (parts per 10 million).
* rm: average number of rooms per dwelling.
* age: proportion of owner-occupied units built prior to 1940.
* dis: weighted mean of distances to five Boston employment centres.
* rad: index of accessibility to radial highways.
* tax: full-value property-tax rate per $10,000.
* ptratio: pupil-teacher ratio by town.
* black: 1000(Bk-0.63)E2 where Bk is the proportion of blacks by town.
* lstat: lower status of the population (percent).
* medv: median value of owner-occupied homes in $1000s.


```{r, message=FALSE}

library(tidyr)
library(readr)
library(dplyr)
library(ggplot2)
library(GGally)
library(MASS)
library(corrplot)

# load the data from the MASS package
data("Boston")

# explore the dataset
str(Boston)
summary(Boston)

#checking if I can extract some valuable info from pairs and ggplots
pairs (Boston)

p_boston <- ggpairs(Boston, mapping = aes(alpha=0.3),
             lower = list(combo = wrap("facethist", bins = 20)))

p_boston

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) 


# visualize the correlation matrix
corrplot(cor_matrix, method="circle")

print(cor_matrix)

```
By examining the correlation matrix and the corrplot, you can gain insights into the relationships between our different variables. Positive correlations (values closer to 1) indicate a positive relationship, negative correlations (values closer to -1) indicate a negative relationship, and values close to 0 suggest weak or no linear relationship. 

```{r}

# calculate the correlation matrix and round it using the pipe (%>%)
cor_matrix <- cor(Boston) %>% round(2)

# print the rounded correlation matrix
print(cor_matrix)

# visualize the rounded correlation matrix with corrplot
library(corrplot)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

```


The Boston data contains only numerical values, so we can use the function 'scale()' to standardize the whole dataset.
I created a categorical variable of the crime rate in the Boston dataset from a continuous one. The variables have been successfully standardized, a categorical variable 'crime' has been created, and the dataset is split into training and testing set. Each variable has been standardized, with a mean of 0 and a standard deviation of 1.
The minimum, maximum, and quartile values for each variable are now on a standardized scale.
The crime rate has been categorized into four groups based.
There are 127 observations in the category [-0.419, -0.411], 126 in (-0.411, -0.39], 126 in (-0.39, 0.00739], and 127 in (0.00739, 9.92].

```{r}

# center and standardize variables
boston_scaled <-  scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE)

# look at the table of the new factor crime
table(crime)

# adjust the cut() function with labels
labels <- c("low", "med_low", "med_high", "high")
crime <- cut(boston_scaled$crim, breaks = bins, labels = labels, include.lowest = TRUE)

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)


```



```{r}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)


```
The LDA model has been trained on the training data and provides information on how predictor variables contribute to the classification of crime groups. LD1 explains the majority of the variance.
The confusion matrix indicates how well the model performs on the test dataset, with a count of correct and incorrect predictions for each crime group.

The probabilities of each group occurring in the training data before considering any predictor, can be extracted from the Prior Probabilities of Groups. We have 25.74% of probability of the crime goup beeing 'low', 27.23% for med_low, 23.51% for 'med_high' and 23.51% for high.

For the group means, the values represent the mean standardized values for each predictor variable within each crime group. For example, in the "low" crime group, the average value of zn is approximately 0.97. 
For the coefficients of linear discriminants, the coefficients represent the loadings of each predictor variable on the linear discriminant functions (LD1, LD2, LD3). Positive coefficients means a positive association with the corresponding discriminant function.

The propotion of trace is showing the proportion of the total variance explained by each discriminant function.
In our cae, LD1 explains approximately 95.22% of the variance, LD2 explains 3.55% and LD3 explains 1.23%.

The confusion matrix compares the predicted classes (low, med_low, med_high, high) with the correct classes in the test dataset. Our diagonal represents correct predictions, and off-diagonal elements represent misclassifications.
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results 
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)


```


I calculated the total within-cluster sum of squares (twcss) for different numbers of clusters and plot the results to find the elbow point.
The for visualizing the Clusters I've run the k-means algorithm again with the optimal number of clusters and visualize the clusters using the pairs() function.  The pairs() function helped me to visualize the clusters in a scatterplot matrix. The color of points represents the cluster membership.
Each point in the scatterplot matrix corresponds to an observation, and the grouping is based on the k-means clustering. The clustering helps us identify groups of observations that are similar in terms of standardized variables
We can observe how observations within each cluster are similar to each other, and there is a clear separation between clusters.


```{r}

data("Boston")

# Standardize the dataset
boston_scaled <- scale(Boston)

# Calculate distances between observations
distances <- dist(boston_scaled)

k_means_result <- kmeans(boston_scaled, centers = 3)

k_max <- 10
twcss <- sapply(1:k_max, function(k) kmeans(boston_scaled, centers = k)$tot.withinss)

plot(1:k_max, twcss, type = 'b', main = 'Total WCSS vs. Number of Clusters', xlab = 'Number of Clusters', ylab = 'Total WCSS')

# Run k-means again with the optimal number of clusters
optimal_k <- 3
k_means_optimal <- kmeans(boston_scaled, centers = optimal_k)

# Visualize the clusters with pairs() function, using first 5 variables
pairs(boston_scaled[, 1:5], col = k_means_optimal$cluster)

```



Bonus

```{r}
# k-means clustering
km <- kmeans(Boston, centers = 4)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

set.seed(123)

# determine the number of clusters
k_max <- 4

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)


```

Super Bonus

Given code in the exercise:
```{r}

library(plotly)

model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')

```

Modified code:

The dimensions of your data might be causing the issue. The number of rows in your matrix_product is 404, but the color vector has a length of 506. We need to make sure that the vectors used for x, y, and z have the same length as the color vector.

```{r, message=FALSE}

library(plotly)

#set.seed(123)
#k_max <- 3  
#km <- kmeans(Boston, centers = k_max)
model_predictors <- dplyr::select(train, -crime)

# Matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

# Check the dimensions
dim(matrix_product)
length(train$crime)

# Plot 3D scatter plot with color based on crime classes
plot_ly(data = matrix_product, x = ~LD1, y = ~LD2, z = ~LD3, 
        type = 'scatter3d', mode = 'markers', color = as.factor(train$crime),
        marker = list(size = 5))  # Adjust marker size as needed

```